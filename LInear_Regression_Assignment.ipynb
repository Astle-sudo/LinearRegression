{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WYQntfQwNdM1"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import numpy\n",
        "import warnings\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ HELPER FUNCTIONS ------------------------------------\n",
        "# - The multicollinearity function returns the VIF values of the features\n",
        "# - The TrainedModel function trains a linear regression model on the provided features and labels\n",
        "# - The find_most_correlated_columns function does what the name suggests, it takes in a correlation\n",
        "#   matrix as the input.\n",
        "# - The relevantFeatures function gives the feature lists, sorted according to it's relevance\n",
        "\n",
        "def multicollinearity(X, y, threshold=5):\n",
        "    vif_values = pandas.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)\n",
        "    return vif_values\n",
        "\n",
        "def TrainedModel (X, y) :\n",
        "  model = LinearRegression()\n",
        "  model.fit(X, y)\n",
        "  return model\n",
        "\n",
        "def find_most_correlated_columns(corr_matrix, threshold=0.6):\n",
        "    corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            col1 = corr_matrix.columns[i]\n",
        "            col2 = corr_matrix.columns[j]\n",
        "            corr_value = abs(corr_matrix.loc[col1, col2])\n",
        "            if corr_value >= threshold:\n",
        "                corr_pairs.append((col1, col2, corr_value))\n",
        "    return sorted(corr_pairs, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "# In order to select the most relevant features, a lieanr regresson is run with each feature\n",
        "# and label, and the F-statistic and p-value are noted. The features are than sorted according\n",
        "# to these values.\n",
        "\n",
        "def relevantFeatures (X, y) :\n",
        "  feature_variances = {}\n",
        "  for column in X.columns:\n",
        "    feature = X[[column]]\n",
        "    F, p = f_regression(feature, y)\n",
        "    feature_variances[column] = F[0]\n",
        "\n",
        "  sorted_features = sorted(feature_variances.items(), key=lambda x: x[1], reverse=True)\n",
        "  for feature, variance in sorted_features:\n",
        "    print(f\"{feature}: {variance:.2f}\")"
      ],
      "metadata": {
        "id": "BdhqoSJwg74f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After importing the data, some unnecessary columns are dropped. The above mentioned functions\n",
        "# are used to find out which features are most relevant and which ones are most correlated with\n",
        "# the label, count. The three selected features are registered, casual and temp+atemp (since there's\n",
        "# also a high multicllinearity between temp and atemp).\n",
        "\n",
        "data = pandas.read_csv('Data.csv')\n",
        "columns_to_exclude = ['instant', 'dteday']\n",
        "columns_to_encode = ['season', 'weekday', 'month']\n",
        "data = data.drop(columns_to_exclude, axis=1)\n",
        "data = pandas.get_dummies(data, columns=columns_to_encode, drop_first=True)\n",
        "\n",
        "for i in find_most_correlated_columns(data.corr()) :\n",
        "  print(i)\n",
        "print()\n",
        "\n",
        "relevantFeatures(data.drop(columns=['count']), data['count'])"
      ],
      "metadata": {
        "id": "jAsSRC0-O3W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the selected features, a multiple regression model is trained\n",
        "\n",
        "X = pandas.DataFrame()\n",
        "X['overallTemp'] = (data['temp'] + data['atemp']) / 2\n",
        "X['registered'] = data['registered']\n",
        "X['casual'] = data['casual']\n",
        "y = data['count']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "u3Pc49ECO91i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The predicted values are compared in order to gauge the model efficiency.\n",
        "# The R-squared and MSE values are calculated as well.\n",
        "\n",
        "model = TrainedModel (X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f'R2: {r2_score(y_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_test, y_pred)}')"
      ],
      "metadata": {
        "id": "n43mtOE6qsnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0009bb6d-86b5-4528-816e-74d8202159ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2: 1.0\n",
            "MSE: 7.343352544145842e-24\n"
          ]
        }
      ]
    }
  ]
}